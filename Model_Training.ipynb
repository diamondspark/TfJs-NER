{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Model_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diamondspark/TfJs-NER/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEEYl1vFQvom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6e4100-9b48-4e09-aa14-e66d772f35ec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# !pip install -r /content/gdrive/My\\ Drive/requirements.txt\n",
        "# !pip install tensorflowjs==0.8\n",
        "# !pip install keras == 2.2.2\n",
        "# !pip install tensorflow-gpu==1.15.2\n",
        "import torch\n",
        "print(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "!ls \"/content/gdrive/My Drive/\" \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "cuda\n",
            " data\t\t\t   IMG_20150731_144132.jpg   requirements.gdoc\n",
            " glove.6B.100d.txt\t   IMG_20150731_145814.jpg   requirements.txt\n",
            " group1-shard1of3.bin\t   IMG_20150731_145816.jpg   trained_models\n",
            " group1-shard2of3.bin\t   model.json\t\t    'US Diaries'\n",
            " group1-shard3of3.bin\t   Model_Training.ipynb      vehicles.zip\n",
            " IMG_20150731_144102.jpg  'New folder'\t\t     vocabs.js\n",
            " IMG_20150731_144118.jpg   non-vehicles.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JB0BNAzvrxo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7e1b09-7515-4e94-a139-bfb28a141d42"
      },
      "source": [
        "import keras, tensorflow\n",
        "keras.__version__, tensorflow.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.4.3', '2.3.0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fvz3wivQHNQ"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 113\n",
        "EMBEDDING_DIM = 100 # 50 or 100 or 200 or 300\n",
        "PAD_ID = 0\n",
        "UNK_ID = 1\n",
        "\n",
        "\n",
        "def word_preprocessor(word):\n",
        "    word = re.sub(r'\\d+', '1', re.sub(r\"[-|.|,|\\?|\\!]+\", '', word))\n",
        "    word = word.lower()\n",
        "    if word != '':\n",
        "        return word\n",
        "    else:\n",
        "        return '.'\n",
        "\n",
        "    \n",
        "def load_data(path, word_preprocessor=word_preprocessor):\n",
        "    tags = []\n",
        "    words = []\n",
        "    data = {'words': [], 'tags': []}\n",
        "    with open(path) as f:\n",
        "        for line in f.readlines()[2:]:\n",
        "            if line != '\\n':\n",
        "                parts = line.replace('\\n', '').split(' ')\n",
        "                words.append(word_preprocessor(parts[0]))\n",
        "                if 'MISC' in parts[-1]:\n",
        "                    tags.append(parts[-1][-4:])\n",
        "                else:\n",
        "                    tags.append(parts[-1][-3:])\n",
        "            else:\n",
        "                data['words'].append(words)\n",
        "                data['tags'].append(tags)\n",
        "                words, tags = [], []\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def make_vocab(sentences, tags=False):\n",
        "    vocab = {\"<PAD>\": PAD_ID, \"<UNK>\": UNK_ID}\n",
        "    idd = max([PAD_ID, UNK_ID]) + 1\n",
        "    for sen in sentences:\n",
        "        for word in sen:\n",
        "            if word not in vocab:\n",
        "                vocab[word] = idd\n",
        "                idd += 1\n",
        "                \n",
        "    return vocab\n",
        "\n",
        "\n",
        "def make_sequences(list_of_words, vocab, word_preprocessor=None):\n",
        "    sequences = []\n",
        "    for words in list_of_words:\n",
        "        seq = []\n",
        "        for word in words:\n",
        "            if word_preprocessor:\n",
        "                word = word_preprocessor(word)\n",
        "            seq.append(vocab.get(word, UNK_ID))\n",
        "        sequences.append(seq)\n",
        "    return sequences\n",
        "\n",
        "\n",
        "def make_embedding_tensor(glova_path, words_vocab):\n",
        "    \"\"\"\n",
        "        We use GloVe 6B 100d.\n",
        "        You can download it from: https://nlp.stanford.edu/projects/glove/\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "    with open(os.path.join(glova_path, f\"glove.6B.{EMBEDDING_DIM}d.txt\")) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_tensor = np.zeros((len(words_vocab) + 1, EMBEDDING_DIM))\n",
        "    for word, i in words_vocab.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_tensor[i] = embedding_vector\n",
        "    \n",
        "    return embedding_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQvvEfEePtF6"
      },
      "source": [
        "from tensorflow.keras.layers import (GRU,Dense, Dropout, Embedding, Flatten,\n",
        "                                     Input, Multiply, Permute, RepeatVector,\n",
        "                                     Softmax, Bidirectional)\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "\n",
        "# from utils import MAX_SEQUENCE_LENGTH\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "def make_ner_model(embedding_tensor, words_vocab_size, tags_vocab_size,\n",
        "                   num_hidden_units=128*2, attention_units=64*2):\n",
        "    EMBEDDING_DIM = embedding_tensor.shape[1]\n",
        "    #MAX_SEQUENCE_LENGTH = T\n",
        "    words_input = Input(dtype='int32', shape=[MAX_SEQUENCE_LENGTH])\n",
        "    #words_input = [1,T] :1 = batchsize (for simplifying explanation)\n",
        "    #EMBEDDING_DIM = d (200 for used pretrained word2vec)\n",
        "    #embedding_tensor = weights from pretrained embedding. Dim: |Vocab| x d\n",
        "    x = Embedding(words_vocab_size + 1,\n",
        "                    EMBEDDING_DIM,\n",
        "                    weights=[embedding_tensor],\n",
        "                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                    trainable=False)(words_input)\n",
        "    #x = vectorized sentence (Sent_i):[1,T,d] :1 = batchsize (for simplifying explanation)\n",
        "    print(x.shape)\n",
        "    #num_hidden_units = h\n",
        "    outputs = GRU(num_hidden_units,\n",
        "                    return_sequences=True,reset_after=not True,\n",
        "                    name='RNN_Layer')(x) \n",
        "    outputs1 = GRU(num_hidden_units,return_sequences=True,reset_after=not True)(outputs)\n",
        "    print(outputs.shape, outputs1.shape)\n",
        "    #outputs = [1,T,h] :1 = batchsize\n",
        "\n",
        "    # Simple attention\n",
        "    hidden_layer = Dense(attention_units, activation='tanh')(outputs1)\n",
        "    hidden_layer = Dropout(0.25)(hidden_layer)\n",
        "    hidden_layer = Dense(1, activation=None)(hidden_layer)\n",
        "    hidden_layer = Flatten()(hidden_layer)\n",
        "    attention_vector = Softmax(name='attention_vector')(hidden_layer)\n",
        "    attention = RepeatVector(num_hidden_units)(attention_vector)\n",
        "    attention = Permute([2, 1])(attention)\n",
        "    encoding = Multiply()([outputs, attention])\n",
        "\n",
        "    encoding = Dropout(0.25)(encoding)\n",
        "    ft1 = Dense(num_hidden_units)(encoding)\n",
        "    ft1 = Dropout(0.25)(ft1)\n",
        "    ft2 = Dense(tags_vocab_size)(ft1)\n",
        "    out = Softmax(name='Final_Sofmax')(ft2)\n",
        "    model = Model(inputs=words_input, outputs=out)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOUxhvzvPtGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9f5630-3649-46d7-c60c-2c3b72c419b3"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "!pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "print(tfjs.__version__)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# from model import make_ner_model\n",
        "# from utils import (MAX_SEQUENCE_LENGTH, PAD_ID, load_data,\n",
        "#                    make_embedding_tensor, make_sequences, make_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/c8/c52e21c49b3baf0845e395241046a993e244dd4b94c9827a8cd2d9b18927/tensorflowjs-2.7.0-py3-none-any.whl (62kB)\n",
            "\r\u001b[K     |█████▎                          | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20kB 30.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 30kB 22.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40kB 20.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 51kB 21.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 61kB 15.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Collecting tensorflow-hub<0.10,>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/83/a7df82744a794107641dad1decaad017d82e25f0e1f761ac9204829eef96/tensorflow_hub-0.9.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.12.4)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.35.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.33.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow<3,>=2.1.0->tensorflowjs) (50.3.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.0.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2020.6.20)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Installing collected packages: tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: tensorflow-hub 0.10.0\n",
            "    Uninstalling tensorflow-hub-0.10.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.10.0\n",
            "Successfully installed tensorflow-hub-0.9.0 tensorflowjs-2.7.0\n",
            "2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2kloIjJPtGh"
      },
      "source": [
        "# !unzip glove.6B.zip -d /content/gdrive/My Drive/Colab Notebooks/glove.6B\n",
        "# !pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjXHSHX2PtGs"
      },
      "source": [
        "train_data = load_data( '/content/gdrive/My Drive/data/train.txt')\n",
        "valid_data = load_data( '/content/gdrive/My Drive/data/valid.txt')\n",
        "\n",
        "words_vocab = make_vocab(train_data['words'])\n",
        "tags_vocab = make_vocab(train_data['tags'])\n",
        "\n",
        "train_data['words_sequences'] = make_sequences(train_data['words'], words_vocab)\n",
        "valid_data['words_sequences'] = make_sequences(valid_data['words'], words_vocab)\n",
        "\n",
        "train_data['tags_sequences'] = make_sequences(train_data['tags'], tags_vocab)\n",
        "valid_data['tags_sequences'] = make_sequences(valid_data['tags'], tags_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fmd2A_nPtG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9560177-4bb4-4791-f21d-152567c2ec1c"
      },
      "source": [
        "train_X = pad_sequences(train_data['words_sequences'],\n",
        "                            maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                            value=PAD_ID, padding='post',\n",
        "                            truncating='post')\n",
        "valid_X = pad_sequences(valid_data['words_sequences'],\n",
        "                        maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                        value=PAD_ID,\n",
        "                        padding='post',\n",
        "                        truncating='post')\n",
        "\n",
        "train_y = pad_sequences(train_data['tags_sequences'],\n",
        "                        maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                        value=PAD_ID,\n",
        "                        padding='post',\n",
        "                        truncating='post')\n",
        "valid_y = pad_sequences(valid_data['tags_sequences'],\n",
        "                        maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                        value=PAD_ID,\n",
        "                        padding='post',\n",
        "                        truncating='post')\n",
        "\n",
        "train_y = to_categorical(train_y)\n",
        "valid_y = to_categorical(valid_y)\n",
        "\n",
        "\n",
        "# def recall_m(y_true, y_pred):\n",
        "#     y_true = K.ones_like(y_true) \n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     all_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    \n",
        "#     recall = true_positives / (all_positives + K.epsilon())\n",
        "#     return recall\n",
        "\n",
        "# def precision_m(y_true, y_pred):\n",
        "#     y_true = K.ones_like(y_true) \n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    \n",
        "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#     return precision\n",
        "\n",
        "# def f1_score(y_true, y_pred):\n",
        "#     precision = precision_m(y_true, y_pred)\n",
        "#     recall = recall_m(y_true, y_pred)\n",
        "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "embedding_tensor = make_embedding_tensor('/content/gdrive/My Drive/', words_vocab)\n",
        "model = make_ner_model(embedding_tensor,\n",
        "                       len(words_vocab), len(tags_vocab))\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='Adam',\n",
        "    metrics=['categorical_accuracy']\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 113, 100)\n",
            "WARNING:tensorflow:Layer RNN_Layer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "(None, 113, 256) (None, 113, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstlXMdHPtG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25afcc3-4f9e-4bab-8e48-6b79225439e2"
      },
      "source": [
        "model.summary()\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 113)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 113, 100)     1719900     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "RNN_Layer (GRU)                 (None, 113, 256)     274176      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       (None, 113, 256)     393984      RNN_Layer[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 113, 128)     32896       gru[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 113, 128)     0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 113, 1)       129         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 113)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_vector (Softmax)      (None, 113)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 256, 113)     0           attention_vector[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 113, 256)     0           repeat_vector[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 113, 256)     0           RNN_Layer[0][0]                  \n",
            "                                                                 permute[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 113, 256)     0           multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 113, 256)     65792       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 113, 256)     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 113, 7)       1799        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Final_Sofmax (Softmax)          (None, 113, 7)       0           dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,488,676\n",
            "Trainable params: 768,776\n",
            "Non-trainable params: 1,719,900\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMknisGPPtHE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6373addf-1122-4c85-f40f-26f6706e47a8"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint('/content/gdrive/My Drive/trained_models/n_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# K.clear_session()\n",
        "model.fit(train_X, train_y,\n",
        "          epochs=500,\n",
        "          batch_size=1024,\n",
        "          validation_data=(valid_X, valid_y),\n",
        "        callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.7784 - categorical_accuracy: 0.9096\n",
            "Epoch 00001: val_loss improved from inf to 1.50358, saving model to /content/gdrive/My Drive/trained_models/n_weights.01-1.50.h5\n",
            "15/15 [==============================] - 8s 514ms/step - loss: 1.7784 - categorical_accuracy: 0.9096 - val_loss: 1.5036 - val_categorical_accuracy: 0.9608\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.1494 - categorical_accuracy: 0.9126\n",
            "Epoch 00002: val_loss improved from 1.50358 to 0.64909, saving model to /content/gdrive/My Drive/trained_models/n_weights.02-0.65.h5\n",
            "15/15 [==============================] - 7s 479ms/step - loss: 1.1494 - categorical_accuracy: 0.9126 - val_loss: 0.6491 - val_categorical_accuracy: 0.8683\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.4410 - categorical_accuracy: 0.8792\n",
            "Epoch 00003: val_loss improved from 0.64909 to 0.35251, saving model to /content/gdrive/My Drive/trained_models/n_weights.03-0.35.h5\n",
            "15/15 [==============================] - 7s 496ms/step - loss: 0.4410 - categorical_accuracy: 0.8792 - val_loss: 0.3525 - val_categorical_accuracy: 0.8682\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.3139 - categorical_accuracy: 0.8791\n",
            "Epoch 00004: val_loss improved from 0.35251 to 0.32187, saving model to /content/gdrive/My Drive/trained_models/n_weights.04-0.32.h5\n",
            "15/15 [==============================] - 7s 482ms/step - loss: 0.3139 - categorical_accuracy: 0.8791 - val_loss: 0.3219 - val_categorical_accuracy: 0.8682\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2867 - categorical_accuracy: 0.8791\n",
            "Epoch 00005: val_loss improved from 0.32187 to 0.29667, saving model to /content/gdrive/My Drive/trained_models/n_weights.05-0.30.h5\n",
            "15/15 [==============================] - 7s 497ms/step - loss: 0.2867 - categorical_accuracy: 0.8791 - val_loss: 0.2967 - val_categorical_accuracy: 0.8683\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2649 - categorical_accuracy: 0.8791\n",
            "Epoch 00006: val_loss improved from 0.29667 to 0.27685, saving model to /content/gdrive/My Drive/trained_models/n_weights.06-0.28.h5\n",
            "15/15 [==============================] - 7s 476ms/step - loss: 0.2649 - categorical_accuracy: 0.8791 - val_loss: 0.2768 - val_categorical_accuracy: 0.8683\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2492 - categorical_accuracy: 0.8792\n",
            "Epoch 00007: val_loss improved from 0.27685 to 0.26238, saving model to /content/gdrive/My Drive/trained_models/n_weights.07-0.26.h5\n",
            "15/15 [==============================] - 7s 496ms/step - loss: 0.2492 - categorical_accuracy: 0.8792 - val_loss: 0.2624 - val_categorical_accuracy: 0.8683\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2360 - categorical_accuracy: 0.8792\n",
            "Epoch 00008: val_loss improved from 0.26238 to 0.24831, saving model to /content/gdrive/My Drive/trained_models/n_weights.08-0.25.h5\n",
            "15/15 [==============================] - 7s 483ms/step - loss: 0.2360 - categorical_accuracy: 0.8792 - val_loss: 0.2483 - val_categorical_accuracy: 0.8683\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2232 - categorical_accuracy: 0.8792\n",
            "Epoch 00009: val_loss improved from 0.24831 to 0.23436, saving model to /content/gdrive/My Drive/trained_models/n_weights.09-0.23.h5\n",
            "15/15 [==============================] - 7s 489ms/step - loss: 0.2232 - categorical_accuracy: 0.8792 - val_loss: 0.2344 - val_categorical_accuracy: 0.8683\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.2097 - categorical_accuracy: 0.8792\n",
            "Epoch 00010: val_loss improved from 0.23436 to 0.21956, saving model to /content/gdrive/My Drive/trained_models/n_weights.10-0.22.h5\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.2097 - categorical_accuracy: 0.8792 - val_loss: 0.2196 - val_categorical_accuracy: 0.8683\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1966 - categorical_accuracy: 0.8792\n",
            "Epoch 00011: val_loss improved from 0.21956 to 0.20529, saving model to /content/gdrive/My Drive/trained_models/n_weights.11-0.21.h5\n",
            "15/15 [==============================] - 7s 493ms/step - loss: 0.1966 - categorical_accuracy: 0.8792 - val_loss: 0.2053 - val_categorical_accuracy: 0.8683\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1817 - categorical_accuracy: 0.9124\n",
            "Epoch 00012: val_loss improved from 0.20529 to 0.18905, saving model to /content/gdrive/My Drive/trained_models/n_weights.12-0.19.h5\n",
            "15/15 [==============================] - 7s 479ms/step - loss: 0.1817 - categorical_accuracy: 0.9124 - val_loss: 0.1890 - val_categorical_accuracy: 0.9691\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1671 - categorical_accuracy: 0.9690\n",
            "Epoch 00013: val_loss improved from 0.18905 to 0.17557, saving model to /content/gdrive/My Drive/trained_models/n_weights.13-0.18.h5\n",
            "15/15 [==============================] - 7s 494ms/step - loss: 0.1671 - categorical_accuracy: 0.9690 - val_loss: 0.1756 - val_categorical_accuracy: 0.9692\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1514 - categorical_accuracy: 0.9746\n",
            "Epoch 00014: val_loss improved from 0.17557 to 0.16129, saving model to /content/gdrive/My Drive/trained_models/n_weights.14-0.16.h5\n",
            "15/15 [==============================] - 7s 493ms/step - loss: 0.1514 - categorical_accuracy: 0.9746 - val_loss: 0.1613 - val_categorical_accuracy: 0.9742\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1350 - categorical_accuracy: 0.9778\n",
            "Epoch 00015: val_loss improved from 0.16129 to 0.14753, saving model to /content/gdrive/My Drive/trained_models/n_weights.15-0.15.h5\n",
            "15/15 [==============================] - 7s 487ms/step - loss: 0.1350 - categorical_accuracy: 0.9778 - val_loss: 0.1475 - val_categorical_accuracy: 0.9759\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1237 - categorical_accuracy: 0.9784\n",
            "Epoch 00016: val_loss improved from 0.14753 to 0.13339, saving model to /content/gdrive/My Drive/trained_models/n_weights.16-0.13.h5\n",
            "15/15 [==============================] - 7s 491ms/step - loss: 0.1237 - categorical_accuracy: 0.9784 - val_loss: 0.1334 - val_categorical_accuracy: 0.9767\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1153 - categorical_accuracy: 0.9788\n",
            "Epoch 00017: val_loss improved from 0.13339 to 0.12754, saving model to /content/gdrive/My Drive/trained_models/n_weights.17-0.13.h5\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.1153 - categorical_accuracy: 0.9788 - val_loss: 0.1275 - val_categorical_accuracy: 0.9762\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1094 - categorical_accuracy: 0.9790\n",
            "Epoch 00018: val_loss improved from 0.12754 to 0.12015, saving model to /content/gdrive/My Drive/trained_models/n_weights.18-0.12.h5\n",
            "15/15 [==============================] - 7s 487ms/step - loss: 0.1094 - categorical_accuracy: 0.9790 - val_loss: 0.1201 - val_categorical_accuracy: 0.9772\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1051 - categorical_accuracy: 0.9792\n",
            "Epoch 00019: val_loss improved from 0.12015 to 0.11687, saving model to /content/gdrive/My Drive/trained_models/n_weights.19-0.12.h5\n",
            "15/15 [==============================] - 7s 489ms/step - loss: 0.1051 - categorical_accuracy: 0.9792 - val_loss: 0.1169 - val_categorical_accuracy: 0.9767\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.1021 - categorical_accuracy: 0.9792\n",
            "Epoch 00020: val_loss improved from 0.11687 to 0.11493, saving model to /content/gdrive/My Drive/trained_models/n_weights.20-0.11.h5\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.1021 - categorical_accuracy: 0.9792 - val_loss: 0.1149 - val_categorical_accuracy: 0.9772\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0993 - categorical_accuracy: 0.9795\n",
            "Epoch 00021: val_loss improved from 0.11493 to 0.11138, saving model to /content/gdrive/My Drive/trained_models/n_weights.21-0.11.h5\n",
            "15/15 [==============================] - 7s 497ms/step - loss: 0.0993 - categorical_accuracy: 0.9795 - val_loss: 0.1114 - val_categorical_accuracy: 0.9774\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0974 - categorical_accuracy: 0.9795\n",
            "Epoch 00022: val_loss improved from 0.11138 to 0.11013, saving model to /content/gdrive/My Drive/trained_models/n_weights.22-0.11.h5\n",
            "15/15 [==============================] - 7s 481ms/step - loss: 0.0974 - categorical_accuracy: 0.9795 - val_loss: 0.1101 - val_categorical_accuracy: 0.9774\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0956 - categorical_accuracy: 0.9796\n",
            "Epoch 00023: val_loss improved from 0.11013 to 0.10889, saving model to /content/gdrive/My Drive/trained_models/n_weights.23-0.11.h5\n",
            "15/15 [==============================] - 7s 486ms/step - loss: 0.0956 - categorical_accuracy: 0.9796 - val_loss: 0.1089 - val_categorical_accuracy: 0.9774\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0943 - categorical_accuracy: 0.9797\n",
            "Epoch 00024: val_loss improved from 0.10889 to 0.10599, saving model to /content/gdrive/My Drive/trained_models/n_weights.24-0.11.h5\n",
            "15/15 [==============================] - 7s 484ms/step - loss: 0.0943 - categorical_accuracy: 0.9797 - val_loss: 0.1060 - val_categorical_accuracy: 0.9776\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0930 - categorical_accuracy: 0.9797\n",
            "Epoch 00025: val_loss improved from 0.10599 to 0.10497, saving model to /content/gdrive/My Drive/trained_models/n_weights.25-0.10.h5\n",
            "15/15 [==============================] - 7s 496ms/step - loss: 0.0930 - categorical_accuracy: 0.9797 - val_loss: 0.1050 - val_categorical_accuracy: 0.9776\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0919 - categorical_accuracy: 0.9797\n",
            "Epoch 00026: val_loss improved from 0.10497 to 0.10437, saving model to /content/gdrive/My Drive/trained_models/n_weights.26-0.10.h5\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.0919 - categorical_accuracy: 0.9797 - val_loss: 0.1044 - val_categorical_accuracy: 0.9777\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0910 - categorical_accuracy: 0.9798\n",
            "Epoch 00027: val_loss improved from 0.10437 to 0.10266, saving model to /content/gdrive/My Drive/trained_models/n_weights.27-0.10.h5\n",
            "15/15 [==============================] - 7s 480ms/step - loss: 0.0910 - categorical_accuracy: 0.9798 - val_loss: 0.1027 - val_categorical_accuracy: 0.9777\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0902 - categorical_accuracy: 0.9798\n",
            "Epoch 00028: val_loss did not improve from 0.10266\n",
            "15/15 [==============================] - 7s 479ms/step - loss: 0.0902 - categorical_accuracy: 0.9798 - val_loss: 0.1032 - val_categorical_accuracy: 0.9775\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0895 - categorical_accuracy: 0.9798\n",
            "Epoch 00029: val_loss improved from 0.10266 to 0.10183, saving model to /content/gdrive/My Drive/trained_models/n_weights.29-0.10.h5\n",
            "15/15 [==============================] - 7s 482ms/step - loss: 0.0895 - categorical_accuracy: 0.9798 - val_loss: 0.1018 - val_categorical_accuracy: 0.9777\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0888 - categorical_accuracy: 0.9798\n",
            "Epoch 00030: val_loss improved from 0.10183 to 0.10051, saving model to /content/gdrive/My Drive/trained_models/n_weights.30-0.10.h5\n",
            "15/15 [==============================] - 7s 480ms/step - loss: 0.0888 - categorical_accuracy: 0.9798 - val_loss: 0.1005 - val_categorical_accuracy: 0.9778\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0881 - categorical_accuracy: 0.9798\n",
            "Epoch 00031: val_loss improved from 0.10051 to 0.09994, saving model to /content/gdrive/My Drive/trained_models/n_weights.31-0.10.h5\n",
            "15/15 [==============================] - 7s 488ms/step - loss: 0.0881 - categorical_accuracy: 0.9798 - val_loss: 0.0999 - val_categorical_accuracy: 0.9779\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0878 - categorical_accuracy: 0.9798\n",
            "Epoch 00032: val_loss improved from 0.09994 to 0.09950, saving model to /content/gdrive/My Drive/trained_models/n_weights.32-0.10.h5\n",
            "15/15 [==============================] - 7s 477ms/step - loss: 0.0878 - categorical_accuracy: 0.9798 - val_loss: 0.0995 - val_categorical_accuracy: 0.9780\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0871 - categorical_accuracy: 0.9799\n",
            "Epoch 00033: val_loss improved from 0.09950 to 0.09940, saving model to /content/gdrive/My Drive/trained_models/n_weights.33-0.10.h5\n",
            "15/15 [==============================] - 7s 483ms/step - loss: 0.0871 - categorical_accuracy: 0.9799 - val_loss: 0.0994 - val_categorical_accuracy: 0.9780\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0867 - categorical_accuracy: 0.9799\n",
            "Epoch 00034: val_loss improved from 0.09940 to 0.09904, saving model to /content/gdrive/My Drive/trained_models/n_weights.34-0.10.h5\n",
            "15/15 [==============================] - 7s 487ms/step - loss: 0.0867 - categorical_accuracy: 0.9799 - val_loss: 0.0990 - val_categorical_accuracy: 0.9777\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0862 - categorical_accuracy: 0.9800\n",
            "Epoch 00035: val_loss improved from 0.09904 to 0.09814, saving model to /content/gdrive/My Drive/trained_models/n_weights.35-0.10.h5\n",
            "15/15 [==============================] - 7s 481ms/step - loss: 0.0862 - categorical_accuracy: 0.9800 - val_loss: 0.0981 - val_categorical_accuracy: 0.9780\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0851 - categorical_accuracy: 0.9803\n",
            "Epoch 00036: val_loss improved from 0.09814 to 0.09744, saving model to /content/gdrive/My Drive/trained_models/n_weights.36-0.10.h5\n",
            "15/15 [==============================] - 7s 478ms/step - loss: 0.0851 - categorical_accuracy: 0.9803 - val_loss: 0.0974 - val_categorical_accuracy: 0.9782\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0847 - categorical_accuracy: 0.9804\n",
            "Epoch 00037: val_loss did not improve from 0.09744\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0847 - categorical_accuracy: 0.9804 - val_loss: 0.0975 - val_categorical_accuracy: 0.9781\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0877 - categorical_accuracy: 0.9796\n",
            "Epoch 00038: val_loss did not improve from 0.09744\n",
            "15/15 [==============================] - 7s 475ms/step - loss: 0.0877 - categorical_accuracy: 0.9796 - val_loss: 0.1002 - val_categorical_accuracy: 0.9773\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0846 - categorical_accuracy: 0.9802\n",
            "Epoch 00039: val_loss improved from 0.09744 to 0.09685, saving model to /content/gdrive/My Drive/trained_models/n_weights.39-0.10.h5\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.0846 - categorical_accuracy: 0.9802 - val_loss: 0.0969 - val_categorical_accuracy: 0.9781\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0833 - categorical_accuracy: 0.9805\n",
            "Epoch 00040: val_loss did not improve from 0.09685\n",
            "15/15 [==============================] - 7s 481ms/step - loss: 0.0833 - categorical_accuracy: 0.9805 - val_loss: 0.0972 - val_categorical_accuracy: 0.9779\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0826 - categorical_accuracy: 0.9806\n",
            "Epoch 00041: val_loss improved from 0.09685 to 0.09578, saving model to /content/gdrive/My Drive/trained_models/n_weights.41-0.10.h5\n",
            "15/15 [==============================] - 7s 478ms/step - loss: 0.0826 - categorical_accuracy: 0.9806 - val_loss: 0.0958 - val_categorical_accuracy: 0.9783\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0819 - categorical_accuracy: 0.9807\n",
            "Epoch 00042: val_loss improved from 0.09578 to 0.09491, saving model to /content/gdrive/My Drive/trained_models/n_weights.42-0.09.h5\n",
            "15/15 [==============================] - 7s 470ms/step - loss: 0.0819 - categorical_accuracy: 0.9807 - val_loss: 0.0949 - val_categorical_accuracy: 0.9783\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0808 - categorical_accuracy: 0.9808\n",
            "Epoch 00043: val_loss improved from 0.09491 to 0.09362, saving model to /content/gdrive/My Drive/trained_models/n_weights.43-0.09.h5\n",
            "15/15 [==============================] - 7s 480ms/step - loss: 0.0808 - categorical_accuracy: 0.9808 - val_loss: 0.0936 - val_categorical_accuracy: 0.9785\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0798 - categorical_accuracy: 0.9810\n",
            "Epoch 00044: val_loss improved from 0.09362 to 0.09289, saving model to /content/gdrive/My Drive/trained_models/n_weights.44-0.09.h5\n",
            "15/15 [==============================] - 7s 486ms/step - loss: 0.0798 - categorical_accuracy: 0.9810 - val_loss: 0.0929 - val_categorical_accuracy: 0.9785\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0789 - categorical_accuracy: 0.9811\n",
            "Epoch 00045: val_loss improved from 0.09289 to 0.09231, saving model to /content/gdrive/My Drive/trained_models/n_weights.45-0.09.h5\n",
            "15/15 [==============================] - 7s 482ms/step - loss: 0.0789 - categorical_accuracy: 0.9811 - val_loss: 0.0923 - val_categorical_accuracy: 0.9785\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0778 - categorical_accuracy: 0.9812\n",
            "Epoch 00046: val_loss improved from 0.09231 to 0.09106, saving model to /content/gdrive/My Drive/trained_models/n_weights.46-0.09.h5\n",
            "15/15 [==============================] - 7s 476ms/step - loss: 0.0778 - categorical_accuracy: 0.9812 - val_loss: 0.0911 - val_categorical_accuracy: 0.9786\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0765 - categorical_accuracy: 0.9813\n",
            "Epoch 00047: val_loss improved from 0.09106 to 0.08915, saving model to /content/gdrive/My Drive/trained_models/n_weights.47-0.09.h5\n",
            "15/15 [==============================] - 7s 482ms/step - loss: 0.0765 - categorical_accuracy: 0.9813 - val_loss: 0.0891 - val_categorical_accuracy: 0.9787\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0778 - categorical_accuracy: 0.9806\n",
            "Epoch 00048: val_loss did not improve from 0.08915\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0778 - categorical_accuracy: 0.9806 - val_loss: 0.0961 - val_categorical_accuracy: 0.9780\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0810 - categorical_accuracy: 0.9800\n",
            "Epoch 00049: val_loss did not improve from 0.08915\n",
            "15/15 [==============================] - 7s 480ms/step - loss: 0.0810 - categorical_accuracy: 0.9800 - val_loss: 0.0906 - val_categorical_accuracy: 0.9785\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0756 - categorical_accuracy: 0.9811\n",
            "Epoch 00050: val_loss improved from 0.08915 to 0.08625, saving model to /content/gdrive/My Drive/trained_models/n_weights.50-0.09.h5\n",
            "15/15 [==============================] - 7s 479ms/step - loss: 0.0756 - categorical_accuracy: 0.9811 - val_loss: 0.0863 - val_categorical_accuracy: 0.9785\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0714 - categorical_accuracy: 0.9815\n",
            "Epoch 00051: val_loss improved from 0.08625 to 0.08185, saving model to /content/gdrive/My Drive/trained_models/n_weights.51-0.08.h5\n",
            "15/15 [==============================] - 7s 478ms/step - loss: 0.0714 - categorical_accuracy: 0.9815 - val_loss: 0.0818 - val_categorical_accuracy: 0.9786\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0746 - categorical_accuracy: 0.9806\n",
            "Epoch 00052: val_loss did not improve from 0.08185\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0746 - categorical_accuracy: 0.9806 - val_loss: 0.0917 - val_categorical_accuracy: 0.9781\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0750 - categorical_accuracy: 0.9809\n",
            "Epoch 00053: val_loss did not improve from 0.08185\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0750 - categorical_accuracy: 0.9809 - val_loss: 0.0869 - val_categorical_accuracy: 0.9783\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0719 - categorical_accuracy: 0.9812\n",
            "Epoch 00054: val_loss did not improve from 0.08185\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0719 - categorical_accuracy: 0.9812 - val_loss: 0.0830 - val_categorical_accuracy: 0.9785\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0685 - categorical_accuracy: 0.9815\n",
            "Epoch 00055: val_loss improved from 0.08185 to 0.07978, saving model to /content/gdrive/My Drive/trained_models/n_weights.55-0.08.h5\n",
            "15/15 [==============================] - 7s 471ms/step - loss: 0.0685 - categorical_accuracy: 0.9815 - val_loss: 0.0798 - val_categorical_accuracy: 0.9786\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0654 - categorical_accuracy: 0.9820\n",
            "Epoch 00056: val_loss improved from 0.07978 to 0.07524, saving model to /content/gdrive/My Drive/trained_models/n_weights.56-0.08.h5\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.0654 - categorical_accuracy: 0.9820 - val_loss: 0.0752 - val_categorical_accuracy: 0.9791\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0623 - categorical_accuracy: 0.9826\n",
            "Epoch 00057: val_loss improved from 0.07524 to 0.07227, saving model to /content/gdrive/My Drive/trained_models/n_weights.57-0.07.h5\n",
            "15/15 [==============================] - 7s 493ms/step - loss: 0.0623 - categorical_accuracy: 0.9826 - val_loss: 0.0723 - val_categorical_accuracy: 0.9795\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0585 - categorical_accuracy: 0.9838\n",
            "Epoch 00058: val_loss improved from 0.07227 to 0.06506, saving model to /content/gdrive/My Drive/trained_models/n_weights.58-0.07.h5\n",
            "15/15 [==============================] - 7s 483ms/step - loss: 0.0585 - categorical_accuracy: 0.9838 - val_loss: 0.0651 - val_categorical_accuracy: 0.9824\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0530 - categorical_accuracy: 0.9853\n",
            "Epoch 00059: val_loss improved from 0.06506 to 0.06064, saving model to /content/gdrive/My Drive/trained_models/n_weights.59-0.06.h5\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.0530 - categorical_accuracy: 0.9853 - val_loss: 0.0606 - val_categorical_accuracy: 0.9837\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0487 - categorical_accuracy: 0.9868\n",
            "Epoch 00060: val_loss improved from 0.06064 to 0.05534, saving model to /content/gdrive/My Drive/trained_models/n_weights.60-0.06.h5\n",
            "15/15 [==============================] - 7s 476ms/step - loss: 0.0487 - categorical_accuracy: 0.9868 - val_loss: 0.0553 - val_categorical_accuracy: 0.9854\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0448 - categorical_accuracy: 0.9881\n",
            "Epoch 00061: val_loss improved from 0.05534 to 0.05164, saving model to /content/gdrive/My Drive/trained_models/n_weights.61-0.05.h5\n",
            "15/15 [==============================] - 7s 482ms/step - loss: 0.0448 - categorical_accuracy: 0.9881 - val_loss: 0.0516 - val_categorical_accuracy: 0.9865\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0419 - categorical_accuracy: 0.9890\n",
            "Epoch 00062: val_loss improved from 0.05164 to 0.04806, saving model to /content/gdrive/My Drive/trained_models/n_weights.62-0.05.h5\n",
            "15/15 [==============================] - 7s 471ms/step - loss: 0.0419 - categorical_accuracy: 0.9890 - val_loss: 0.0481 - val_categorical_accuracy: 0.9874\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0405 - categorical_accuracy: 0.9893\n",
            "Epoch 00063: val_loss did not improve from 0.04806\n",
            "15/15 [==============================] - 7s 472ms/step - loss: 0.0405 - categorical_accuracy: 0.9893 - val_loss: 0.0530 - val_categorical_accuracy: 0.9858\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0390 - categorical_accuracy: 0.9896\n",
            "Epoch 00064: val_loss improved from 0.04806 to 0.04536, saving model to /content/gdrive/My Drive/trained_models/n_weights.64-0.05.h5\n",
            "15/15 [==============================] - 7s 478ms/step - loss: 0.0390 - categorical_accuracy: 0.9896 - val_loss: 0.0454 - val_categorical_accuracy: 0.9878\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0351 - categorical_accuracy: 0.9905\n",
            "Epoch 00065: val_loss improved from 0.04536 to 0.04262, saving model to /content/gdrive/My Drive/trained_models/n_weights.65-0.04.h5\n",
            "15/15 [==============================] - 7s 481ms/step - loss: 0.0351 - categorical_accuracy: 0.9905 - val_loss: 0.0426 - val_categorical_accuracy: 0.9883\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0331 - categorical_accuracy: 0.9909\n",
            "Epoch 00066: val_loss improved from 0.04262 to 0.04061, saving model to /content/gdrive/My Drive/trained_models/n_weights.66-0.04.h5\n",
            "15/15 [==============================] - 7s 486ms/step - loss: 0.0331 - categorical_accuracy: 0.9909 - val_loss: 0.0406 - val_categorical_accuracy: 0.9887\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0314 - categorical_accuracy: 0.9912\n",
            "Epoch 00067: val_loss improved from 0.04061 to 0.03956, saving model to /content/gdrive/My Drive/trained_models/n_weights.67-0.04.h5\n",
            "15/15 [==============================] - 7s 471ms/step - loss: 0.0314 - categorical_accuracy: 0.9912 - val_loss: 0.0396 - val_categorical_accuracy: 0.9888\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0300 - categorical_accuracy: 0.9914\n",
            "Epoch 00068: val_loss did not improve from 0.03956\n",
            "15/15 [==============================] - 7s 475ms/step - loss: 0.0300 - categorical_accuracy: 0.9914 - val_loss: 0.0398 - val_categorical_accuracy: 0.9888\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0290 - categorical_accuracy: 0.9916\n",
            "Epoch 00069: val_loss improved from 0.03956 to 0.03794, saving model to /content/gdrive/My Drive/trained_models/n_weights.69-0.04.h5\n",
            "15/15 [==============================] - 7s 475ms/step - loss: 0.0290 - categorical_accuracy: 0.9916 - val_loss: 0.0379 - val_categorical_accuracy: 0.9890\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0277 - categorical_accuracy: 0.9919\n",
            "Epoch 00070: val_loss improved from 0.03794 to 0.03698, saving model to /content/gdrive/My Drive/trained_models/n_weights.70-0.04.h5\n",
            "15/15 [==============================] - 7s 478ms/step - loss: 0.0277 - categorical_accuracy: 0.9919 - val_loss: 0.0370 - val_categorical_accuracy: 0.9893\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0266 - categorical_accuracy: 0.9922\n",
            "Epoch 00071: val_loss improved from 0.03698 to 0.03597, saving model to /content/gdrive/My Drive/trained_models/n_weights.71-0.04.h5\n",
            "15/15 [==============================] - 7s 478ms/step - loss: 0.0266 - categorical_accuracy: 0.9922 - val_loss: 0.0360 - val_categorical_accuracy: 0.9895\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0257 - categorical_accuracy: 0.9924\n",
            "Epoch 00072: val_loss improved from 0.03597 to 0.03552, saving model to /content/gdrive/My Drive/trained_models/n_weights.72-0.04.h5\n",
            "15/15 [==============================] - 7s 484ms/step - loss: 0.0257 - categorical_accuracy: 0.9924 - val_loss: 0.0355 - val_categorical_accuracy: 0.9897\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0248 - categorical_accuracy: 0.9927\n",
            "Epoch 00073: val_loss improved from 0.03552 to 0.03544, saving model to /content/gdrive/My Drive/trained_models/n_weights.73-0.04.h5\n",
            "15/15 [==============================] - 7s 481ms/step - loss: 0.0248 - categorical_accuracy: 0.9927 - val_loss: 0.0354 - val_categorical_accuracy: 0.9897\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0241 - categorical_accuracy: 0.9930\n",
            "Epoch 00074: val_loss improved from 0.03544 to 0.03443, saving model to /content/gdrive/My Drive/trained_models/n_weights.74-0.03.h5\n",
            "15/15 [==============================] - 7s 473ms/step - loss: 0.0241 - categorical_accuracy: 0.9930 - val_loss: 0.0344 - val_categorical_accuracy: 0.9901\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0233 - categorical_accuracy: 0.9933\n",
            "Epoch 00075: val_loss did not improve from 0.03443\n",
            "15/15 [==============================] - 7s 468ms/step - loss: 0.0233 - categorical_accuracy: 0.9933 - val_loss: 0.0346 - val_categorical_accuracy: 0.9902\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0227 - categorical_accuracy: 0.9935\n",
            "Epoch 00076: val_loss did not improve from 0.03443\n",
            "15/15 [==============================] - 7s 464ms/step - loss: 0.0227 - categorical_accuracy: 0.9935 - val_loss: 0.0345 - val_categorical_accuracy: 0.9902\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0218 - categorical_accuracy: 0.9938\n",
            "Epoch 00077: val_loss improved from 0.03443 to 0.03386, saving model to /content/gdrive/My Drive/trained_models/n_weights.77-0.03.h5\n",
            "15/15 [==============================] - 7s 481ms/step - loss: 0.0218 - categorical_accuracy: 0.9938 - val_loss: 0.0339 - val_categorical_accuracy: 0.9908\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0221 - categorical_accuracy: 0.9938\n",
            "Epoch 00078: val_loss did not improve from 0.03386\n",
            "15/15 [==============================] - 7s 473ms/step - loss: 0.0221 - categorical_accuracy: 0.9938 - val_loss: 0.0340 - val_categorical_accuracy: 0.9907\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0207 - categorical_accuracy: 0.9942\n",
            "Epoch 00079: val_loss improved from 0.03386 to 0.03236, saving model to /content/gdrive/My Drive/trained_models/n_weights.79-0.03.h5\n",
            "15/15 [==============================] - 7s 484ms/step - loss: 0.0207 - categorical_accuracy: 0.9942 - val_loss: 0.0324 - val_categorical_accuracy: 0.9912\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0198 - categorical_accuracy: 0.9945\n",
            "Epoch 00080: val_loss did not improve from 0.03236\n",
            "15/15 [==============================] - 7s 481ms/step - loss: 0.0198 - categorical_accuracy: 0.9945 - val_loss: 0.0330 - val_categorical_accuracy: 0.9912\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0192 - categorical_accuracy: 0.9947\n",
            "Epoch 00081: val_loss improved from 0.03236 to 0.03162, saving model to /content/gdrive/My Drive/trained_models/n_weights.81-0.03.h5\n",
            "15/15 [==============================] - 7s 479ms/step - loss: 0.0192 - categorical_accuracy: 0.9947 - val_loss: 0.0316 - val_categorical_accuracy: 0.9916\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0187 - categorical_accuracy: 0.9949\n",
            "Epoch 00082: val_loss did not improve from 0.03162\n",
            "15/15 [==============================] - 7s 469ms/step - loss: 0.0187 - categorical_accuracy: 0.9949 - val_loss: 0.0323 - val_categorical_accuracy: 0.9914\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0181 - categorical_accuracy: 0.9950\n",
            "Epoch 00083: val_loss did not improve from 0.03162\n",
            "15/15 [==============================] - 7s 475ms/step - loss: 0.0181 - categorical_accuracy: 0.9950 - val_loss: 0.0324 - val_categorical_accuracy: 0.9914\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0177 - categorical_accuracy: 0.9952\n",
            "Epoch 00084: val_loss did not improve from 0.03162\n",
            "15/15 [==============================] - 7s 479ms/step - loss: 0.0177 - categorical_accuracy: 0.9952 - val_loss: 0.0317 - val_categorical_accuracy: 0.9916\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0215 - categorical_accuracy: 0.9942\n",
            "Epoch 00085: val_loss did not improve from 0.03162\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0215 - categorical_accuracy: 0.9942 - val_loss: 0.0407 - val_categorical_accuracy: 0.9895\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0241 - categorical_accuracy: 0.9934\n",
            "Epoch 00086: val_loss did not improve from 0.03162\n",
            "15/15 [==============================] - 7s 468ms/step - loss: 0.0241 - categorical_accuracy: 0.9934 - val_loss: 0.0334 - val_categorical_accuracy: 0.9912\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0188 - categorical_accuracy: 0.9949\n",
            "Epoch 00087: val_loss improved from 0.03162 to 0.03141, saving model to /content/gdrive/My Drive/trained_models/n_weights.87-0.03.h5\n",
            "15/15 [==============================] - 7s 469ms/step - loss: 0.0188 - categorical_accuracy: 0.9949 - val_loss: 0.0314 - val_categorical_accuracy: 0.9917\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0172 - categorical_accuracy: 0.9953\n",
            "Epoch 00088: val_loss improved from 0.03141 to 0.03060, saving model to /content/gdrive/My Drive/trained_models/n_weights.88-0.03.h5\n",
            "15/15 [==============================] - 7s 475ms/step - loss: 0.0172 - categorical_accuracy: 0.9953 - val_loss: 0.0306 - val_categorical_accuracy: 0.9918\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0163 - categorical_accuracy: 0.9955\n",
            "Epoch 00089: val_loss improved from 0.03060 to 0.02988, saving model to /content/gdrive/My Drive/trained_models/n_weights.89-0.03.h5\n",
            "15/15 [==============================] - 7s 477ms/step - loss: 0.0163 - categorical_accuracy: 0.9955 - val_loss: 0.0299 - val_categorical_accuracy: 0.9921\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0157 - categorical_accuracy: 0.9957\n",
            "Epoch 00090: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0157 - categorical_accuracy: 0.9957 - val_loss: 0.0301 - val_categorical_accuracy: 0.9920\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0152 - categorical_accuracy: 0.9958\n",
            "Epoch 00091: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 467ms/step - loss: 0.0152 - categorical_accuracy: 0.9958 - val_loss: 0.0301 - val_categorical_accuracy: 0.9920\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0148 - categorical_accuracy: 0.9959\n",
            "Epoch 00092: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 473ms/step - loss: 0.0148 - categorical_accuracy: 0.9959 - val_loss: 0.0308 - val_categorical_accuracy: 0.9919\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0145 - categorical_accuracy: 0.9960\n",
            "Epoch 00093: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0145 - categorical_accuracy: 0.9960 - val_loss: 0.0302 - val_categorical_accuracy: 0.9921\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0142 - categorical_accuracy: 0.9961\n",
            "Epoch 00094: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 470ms/step - loss: 0.0142 - categorical_accuracy: 0.9961 - val_loss: 0.0304 - val_categorical_accuracy: 0.9921\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0140 - categorical_accuracy: 0.9961\n",
            "Epoch 00095: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 472ms/step - loss: 0.0140 - categorical_accuracy: 0.9961 - val_loss: 0.0310 - val_categorical_accuracy: 0.9920\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0138 - categorical_accuracy: 0.9961\n",
            "Epoch 00096: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 461ms/step - loss: 0.0138 - categorical_accuracy: 0.9961 - val_loss: 0.0306 - val_categorical_accuracy: 0.9921\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0134 - categorical_accuracy: 0.9963\n",
            "Epoch 00097: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 469ms/step - loss: 0.0134 - categorical_accuracy: 0.9963 - val_loss: 0.0309 - val_categorical_accuracy: 0.9920\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0129 - categorical_accuracy: 0.9964\n",
            "Epoch 00098: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 465ms/step - loss: 0.0129 - categorical_accuracy: 0.9964 - val_loss: 0.0316 - val_categorical_accuracy: 0.9919\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0126 - categorical_accuracy: 0.9966\n",
            "Epoch 00099: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0126 - categorical_accuracy: 0.9966 - val_loss: 0.0317 - val_categorical_accuracy: 0.9920\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0124 - categorical_accuracy: 0.9966\n",
            "Epoch 00100: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 470ms/step - loss: 0.0124 - categorical_accuracy: 0.9966 - val_loss: 0.0314 - val_categorical_accuracy: 0.9921\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0121 - categorical_accuracy: 0.9967\n",
            "Epoch 00101: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 466ms/step - loss: 0.0121 - categorical_accuracy: 0.9967 - val_loss: 0.0307 - val_categorical_accuracy: 0.9922\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0118 - categorical_accuracy: 0.9968\n",
            "Epoch 00102: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 471ms/step - loss: 0.0118 - categorical_accuracy: 0.9968 - val_loss: 0.0312 - val_categorical_accuracy: 0.9922\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0117 - categorical_accuracy: 0.9967\n",
            "Epoch 00103: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 478ms/step - loss: 0.0117 - categorical_accuracy: 0.9967 - val_loss: 0.0324 - val_categorical_accuracy: 0.9920\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0119 - categorical_accuracy: 0.9967\n",
            "Epoch 00104: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 468ms/step - loss: 0.0119 - categorical_accuracy: 0.9967 - val_loss: 0.0315 - val_categorical_accuracy: 0.9921\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0112 - categorical_accuracy: 0.9969\n",
            "Epoch 00105: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 469ms/step - loss: 0.0112 - categorical_accuracy: 0.9969 - val_loss: 0.0313 - val_categorical_accuracy: 0.9923\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0110 - categorical_accuracy: 0.9969\n",
            "Epoch 00106: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 474ms/step - loss: 0.0110 - categorical_accuracy: 0.9969 - val_loss: 0.0323 - val_categorical_accuracy: 0.9920\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0107 - categorical_accuracy: 0.9970\n",
            "Epoch 00107: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 473ms/step - loss: 0.0107 - categorical_accuracy: 0.9970 - val_loss: 0.0329 - val_categorical_accuracy: 0.9920\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0104 - categorical_accuracy: 0.9971\n",
            "Epoch 00108: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 470ms/step - loss: 0.0104 - categorical_accuracy: 0.9971 - val_loss: 0.0331 - val_categorical_accuracy: 0.9921\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0102 - categorical_accuracy: 0.9972\n",
            "Epoch 00109: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 463ms/step - loss: 0.0102 - categorical_accuracy: 0.9972 - val_loss: 0.0323 - val_categorical_accuracy: 0.9922\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0099 - categorical_accuracy: 0.9973\n",
            "Epoch 00110: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 473ms/step - loss: 0.0099 - categorical_accuracy: 0.9973 - val_loss: 0.0332 - val_categorical_accuracy: 0.9920\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - ETA: 0s - loss: 0.0098 - categorical_accuracy: 0.9973\n",
            "Epoch 00111: val_loss did not improve from 0.02988\n",
            "15/15 [==============================] - 7s 471ms/step - loss: 0.0098 - categorical_accuracy: 0.9973 - val_loss: 0.0341 - val_categorical_accuracy: 0.9920\n",
            "Epoch 112/500\n",
            " 8/15 [===============>..............] - ETA: 2s - loss: 0.0096 - categorical_accuracy: 0.9974"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-66de5eec0eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         callbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZLwwQYfPtHL"
      },
      "source": [
        "def export_model(model, words_vocab, tags_vocab, site_path):\n",
        "    tfjs.converters.save_keras_model(\n",
        "        model,\n",
        "        os.path.join(site_path, '/content/gdrive/My Drive/')\n",
        "        )\n",
        "\n",
        "    with open(os.path.join(site_path, \"/content/gdrive/My Drive/vocabs.js\"), 'w') as f:\n",
        "        f.write('const words_vocab = {\\n')\n",
        "        for l in json.dumps(words_vocab)[1:-1].split(\",\"):\n",
        "            f.write(\"\\t\"+l+',\\n')\n",
        "        f.write('};\\n')\n",
        "        \n",
        "        f.write('const tags_vocab = {\\n')\n",
        "        for l in json.dumps(tags_vocab)[1:-1].split(\",\"):\n",
        "            f.write(\"\\t\"+l+',\\n')\n",
        "        f.write('};')\n",
        "    print('model exported to ', site_path)\n",
        "# load_model('./trained_models/weights.91-0.03.h5')    \n",
        "# export_model(model, words_vocab, tags_vocab, './')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bldUH5iPtHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9fb0cf-c848-4bed-c7ef-12e2d365c38a"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.is_gpu_available(\n",
        "    cuda_only=False,\n",
        "    min_cuda_compute_capability=None\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-b25d88cf26c7>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMdmWSyAPtHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9b1430-5e9f-4364-8d02-3cde5ef0c24b"
      },
      "source": [
        "print(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpgRoianPtHe"
      },
      "source": [
        "for k in (train_data):\n",
        "    print(k)\n",
        "    print(len(train_data[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tss2R2MPtHh"
      },
      "source": [
        "train_data['words'][21], train_data['words_sequences'][21], train_data['tags_sequences'][21],tags_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcR3W3mFPtHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0cfbec7-4690-4fe3-92f6-a44a3b962a8e"
      },
      "source": [
        "model1 = load_model('/content/gdrive/My Drive/trained_models/n_weights.89-0.03.h5')\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer RNN_Layer will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 113)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 113, 100)     1719900     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "RNN_Layer (GRU)                 (None, 113, 256)     274176      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       (None, 113, 256)     393984      RNN_Layer[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 113, 128)     32896       gru[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 113, 128)     0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 113, 1)       129         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 113)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_vector (Softmax)      (None, 113)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 256, 113)     0           attention_vector[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 113, 256)     0           repeat_vector[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 113, 256)     0           RNN_Layer[0][0]                  \n",
            "                                                                 permute[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 113, 256)     0           multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 113, 256)     65792       dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 113, 256)     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 113, 7)       1799        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Final_Sofmax (Softmax)          (None, 113, 7)       0           dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 2,488,676\n",
            "Trainable params: 768,776\n",
            "Non-trainable params: 1,719,900\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IFchRteafjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adce0810-8a9a-481d-bd95-9ca2748af97f"
      },
      "source": [
        "export_model(model1, words_vocab, tags_vocab, './')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py:123: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  return h5py.File(h5file)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model exported to  ./\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErOR8FtaaoTm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}